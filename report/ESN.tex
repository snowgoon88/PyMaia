\documentclass[12pt]{article}

\usepackage[french]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[OT1]{fontenc}

\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsfonts}


\title{Aperçu d'une architecture de Reservoir Computing\\Les Echo State Networks}
\author{Théo \bsc{Biasutto--Lervat}}
\date{}

\begin{document}
\maketitle

\section{Introduction}

Le \textit{Reservoir computing} (RC) est une alternative à la méthode de descente du gradient pour entraîner les \textit{recurrent neural network} (RNN), une architecture de réseau de neurones artificielles où les connections synaptiques peuvent former des cycles. Cela entraîne l'apparition d'états internes au sein du réseau, lui permettant d'adopter un comportement temporel dynamique.\newline
Un \textit{Echo state network} est un RNN possèdant une couche cachée, le réservoir, avec un faible nombre de synapse dont les poids sont générés aléatoirement. Le poid des neurones de sortie est l'unique sous-ensemble du réseau qui sera entraîné, bien souvent à l'aide d'une régression linéaire.\newline
Pour que l'approche ESN fonctionne, le réservoir doit satisfaire \textit{l'echo state property} : l'état d'un réservoir ne doit dépendre que des données entrées au court du temps (\cite{Jaeger10}). Pour un réservoir assimilant une importante quantitée de donnée, son état interne ne doit donc plus dépendre de l'initialisation.


\section{Formalisation}

Considérons un réseaux de neurones en temps discret, avec K neurones d'entrée, N neurones internes, et L neurones de sortie.\newline
\begin{center}
\includegraphics{esn.jpeg}
\end{center}
\begin{description}
\item[$u(t)$ :] vecteur $\in \mathbb{R}^{K}$, activations des neurones d'entrée à l'instant $t$
\item[$x(t)$ :] vecteur $\in \mathbb{R}^{N}$, activations des neurones internes à l'instant $t$
\item[$y(t)$ :] vecteur $\in \mathbb{R}^{L}$, activations des neurones de sortie à l'instant $t$
\item[$W^{in}$ :] $N \times K$ matrice $\in \mathbb{R}$, poids des synapses d'entrée
\item[$W$ :] $N \times N$ matrice $\in \mathbb{R}$, poids des synapses internes
\item[$W^{out}$ :] $L \times (K + N + L)$ matrice $\in \mathbb{R}$, poids des synapses de sortie
\item[$W^{back}$ :] $N \times L$ matrice $\in \mathbb{R}$, poids des synapses de \textit{feedback}
\item[$\alpha$ :] le taux de fuite
\end{description} 

\section{Fonctionnement}

\subsection{Généralité}
Dans le cas d'un apprentissage supervisé, c'est à dire dont les résultats sont connu à l'avance, voici les différentes équations décrivant le fonctionnement des ESN.\newline
\paragraph{L'état interne}
\begin{center}
$x(t) = f(W^{in}u(t) + Wx(t-1) + W^{back}y(t-1) )$
\end{center}
où $f=(f_{1},...,f_{N})$ sont les fonctions d'activations des neurones du réservoir.\newline
Dans le cas d'un ESN sans synapse de \textit{feedback}, avec biais, taux de fuite et pour fonctions d'activation $\tanh$, nous obtenons
\begin{center}
$x(t) = (1-\alpha)x(t-1) + \alpha\tanh(W^{in}[1;u(t)] + Wx(t-1))$
\end{center}
\paragraph{La sortie}
\begin{center}
$y(t)= W^{out}[1;u(t);x(t)]$
\end{center}
\paragraph{Apprentissage de $W^{out}$}
\begin{center}
$Y = W^{out}M$
\end{center}
avec $M = [1; U; X]$ où $U$ est la matrice des $u(t)$, et $X$ la matrice des $x(t)$. Nous avons donc là un classique problème de régression linéaire que nous allons résoudre avec une \textit{ridge regression}. Nous obtenons donc:
\begin{center}
$W^{out} = Y^{target} M^\top (MM^\top + \beta I)^{-1}$
\end{center}
où $\beta$ est le coefficient de régularisation.

\subsection{Mise en oeuvre}
\begin{enumerate}
\item Générer le réservoir ($W^{in}$, $W$, $\alpha$);
\item Utiliser le corpus d'apprentissage $u(t)$ et collecter $x(t)$;
\item Apprentissage de $W^{out}$;
\item Utiliser les nouvelles données $u(t)$, obtenir  $y(t)$ avec $W^{out}$.
\end{enumerate}


\section{Protocoles}

\subsection{Tâches de génération et de prédiction}

Toutes les tâches explicitées ci-dessous ont pour but d'analyser la faculté d'un réservoir à apprendre de manière supervisée à prédire une séquence temporelle.

\subsubsection{Mackey-Glass}
\paragraph{Formalisation}
\begin{center}
$x(t+1) = \beta \frac{ x(t-\tau) }{ 1+{x(t-\tau)}^n }-\gamma x$
\end{center}
où $\quad \gamma,\beta,n > 0$
\paragraph{Démarche}
L'ESN possède un neurone d'entrée et un neurone de sortie. Après la phase d'apprentissage, le réseau devrait être capable de prédire $y(t)$, ce qui nous ouvre la porte à deux tâches intéressante. D'une part, il est possible de générer la fonction de façon autonome, c'est à dire que nous fournissons au réseau $u(t_{startTask})$ puis les $y(t)$ prédit à l'instant $t$ seront utilisés comme entrées à l'instant $t+1$. D'autre part, nous pouvons simplement prédire la fonction sur une distance $\delta t$, c'est à dire que nous réalisons une tâche de génération où nous réinjectons $u(t)$ à la place de $y(t)$ lorsque $t - t_{startTask} \equiv 0 \pmod{ \Delta t }$.
\paragraph{Mesure de la performance}
Nous nous baserons ici sur la différence entre $y^{target}(t)$ et $y(t)$ pour tout $t$, nous permettant une analyse statistique de l'erreur. L'indice de performance principale que nous utiliserons est la \textit{Root Mean Squared Error}.


\subsubsection{Séquence de lettre}
\paragraph{Formalisation}
\begin{center}
$char(t) = f( char(t-\tau_{1}),..., char(t-\tau_{n}) )$
\end{center}
où $\forall i, \tau_{i} > 0$, $char(t)$ est la fonction retournant la lettre à l'emplacement $t$ et $f$ est la fonction de génération de la séquence.
\paragraph{Génération de la séquence}
Nous générons au hasard $max_{\forall i}(\tau_{i})$ caractères, puis appliquons la fonction $f$ pour générer la suite de la séquence.
\paragraph{Démarche}
Même démarche que pour la fonction de Mackey-Glass, mis à part que l'ESN possède autant de neurone d'entrée et de sortie que de lettres composant la séquence. Ainsi pour une séquence formée à l'aide d'un alphabet de cardinalité 2, le réseau de neurone possèdera deux neurones d'entrée. L'activation du premier neurone correspondant à une lettre, et celle du second neurone correspondant à la deuxième lettre. Bien entendu, la couche de sortie du réseau de neurone ne sera pas activé de façon binaire, et nous regarderons donc l'unité la plus active pour savoir quelle est la lettre prédite par l'ESN.
\paragraph{Mesure de la performance}
Pour cette tâche, nous mesurons la précision (\textit{precision}), le rappel (\textit{recall}), la F-mesure (\textit{F-measure}), et la justesse (\textit{accuracy}) à chaque instant. 
\newline
\newline
$precision(t) = \frac{1}{\#Alphabet} \sum_{l \in Alphabet} \frac{l\ correctement\ predit}{l\ predit}$
\newline
\newline
$rappel(t) = \frac{1}{\#Alphabet} \sum_{l \in Alphabet} \frac{l\ correctement\ predit}{l\ existant}$
\newline
\newline
$F-measure(t) = \frac{2 \times precision(t) \times rappel(t)}{precision(t) + rappel(t)}$
\newline
\newline
$accuracy(t)= \frac{predictions correctes}{t}$

\subsection{Tâche de classification}

Toutes les tâches explicitées ci-dessous ont pour but d'analyser la faculté d'un réservoir à classer des séquences temporelles avec un apprentissage supervisée.

\subsubsection{Trajectoires}
\paragraph{Formalisation}
Nous réutilisons ici les trajectoires décrites dans \cite{HuskenStagge}, c'est à dire les trois équations paramétriques suivantes.
\begin{description}
\item[classe 1:]
\begin{itemize}
\item
\item $x(t) = \alpha \sin(t+\beta)|\sin(t)|$
\item $y(t) = \alpha \cos(t+\beta)|\sin(t)|$
\end{itemize}
\item[classe 2:] 
\begin{itemize}
\item
\item $x(t) = \alpha \sin(\frac{1}{2}t+\beta)\sin(\frac{3}{2}t)$
\item $y(t) = \alpha \cos(t+\beta)\sin(2t)$
\end{itemize}
\item[classe 3:] 
\begin{itemize}
\item
\item $x(t) = \alpha \sin(t+\beta)\sin(2t)$
\item $y(t) = \alpha \cos(t+\beta)\sin(2t)$
\end{itemize}
\end{description}
où $\alpha$ est fixe pour toutes les trajectoires, $\beta$ est choisit aléatoirement entre $[0, 2\pi)$ pour chaque trajectoire. Nous allons échantillonner les trajectoires avec un $\Delta t = \frac{2}{30}\pi$ et un $t_0$ choisit aléatoirement entre $[0, 2\pi)$ pour chaque trajectoire.

\paragraph{Démarche}
Le corpus d'apprentissage et de test est composé de 150 trajectoires (50 par classes) aléatoirement présenté à l'ESN. La réponse du réseau de neurone sera déterminé par l'encodage \"1-of-$N^{(c)}$\", c'est à dire qu'il y aura autant d'unité de sortie que de classe, et que la classe est déterminée par le neurone à la plus forte valeur de sortie.

\paragraph{Mesure de la performance}


\section{Expérimentations}

SOON


\appendix

\section{Paramétrage du réservoir}

\paragraph{Taille du réservoir}
L'entrainement et l'utilisation d'un ESN est peu couteux à côté des autres approches de RNN, ce qui nous permets de créer de grand réservoir, avec un ordre de grandeur de $10^{4}$ neurones. Le réservoir peut cependant être trop gros si la tâche est triviale et qu'il n'y a pas assez de donnée disponible dans le corpus d'apprentissage.\newline
D'après \cite{Lukosevicius12}:
\begin{itemize}
\item Pour des tâches complexes, utiliser un réservoir aussi gros que vous le pouvez.
\item Choisir les paramètres pour de petit réservoir, puis les mettres à l'échelle pour de plus gros.
\item N doit être au moins égal à l'estimation de valeur indépendante que le réservoir doit mémoriser depuis les entrées pour résoudre la tâche.
\end{itemize}

\paragraph{Parcimonie du réservoir}
Une grande partie des éléments de $W^{in}$ doivent être nul. En général, la parcimonie du réservoir affecte peu les performances du réseau de neurone.\newline
D'après \cite{Lukosevicius12}:
\begin{itemize}
\item Connecter chaque neurone à un petit nombres de neurones, indépendament de la taille du réservoir. Profiter de cette parcimonie pour accélerer les calculs.
\end{itemize}

\paragraph{Distribution des éléments non-nuls}
Une grande partie des éléments de $W$ sont nuls, et la distribution des éléments non-nuls peut s'effectuer de différente façon : \textit{symmetrical uniform}, \textit{discrete bi-valued}, \textit{normal distribution centered around zero}, \textit{Gaussian distribution}.\newline
La distribution Gaussienne et normal donne virtuellement les mêmes résultats, la distribution \textit{discrete bi-valued} semble moins efficace, mais peut faciliter l'analyse du réservoir lors de son exécution.

\paragraph{Rayon spectral}
Un des paramètres majeurs d'un ESN est le rayon spectral de la matrice $W$, c'est à dire l'extremum de ses valeurs propres. Généralement, une fois la génération de $W$ effectuée, la matrice est divisée par $\rho(W)$ pour assurer un rayon spectral unitaire. Le rayon spectral détermine à quelle vitesse l'influence d'une entrée disparait du réservoir, et à quel point l'état interne du réservoir est stable.\newline
D'après \cite{Lukosevicius12}:
\begin{itemize}
\item $\rho(W) < 1$ assure dans la propriété \textit(echo state) dans la plupart des situations.
\item Le rayon spectral optimale est souvent plus grand lors de tâche nécessitant une grande mémoire des entrées.
\end{itemize}

\paragraph{Redimensionnement des entrées}
Autre paramètre essentiel d'un ESN, la matrice $W^{in}$ détermine la linéarité de la réponse du réservoir. Pour des tâches très linéaire, $W^{in}$ devrait être peuplé de faible valeur. Lorsque $W^{in}$ possède de forte valeur, les neurones ont tendances à agir comme des commutateurs binaires. De plus, l'échelle de $W^{in}$ et celle de $W$ détermine respectivement comment l'état interne $x(n)$ dépent de l'entrée $u(n)$ et de $x(n-1)$.\newline
D'après \cite{Lukosevicius12}:
\begin{itemize}
\item Mettre à l'échelle $W^{in}$ uniformément, pour limiter le nombre de paramètre de l'ESN. Cependant, par souçi de performance, il est possible de redimmensionner la première colonne de $W^{in}$ (le biais) séparement, et de redimmensionner les autres colonnes séparement si les éléments de $u(n)$ contribue différement à la tâche.
\item Il est conseillé de normaliser les données, ce qui peut aider à borner $u(n)$ et ainsi éviter des aberrations. Par exemple, appliquer $tanh(.)$ à $u(n)$ si celui ci n'est pas borné.
\item Le redimensionnement des entrées régule la quantitée de non-linéarité de $x(n)$ (augmente aussi avec $\rho(W)$), et l'effet de l'entrée courante sur $x(n)$ par rapport aux entrées précédantes (proportionnellement à $\rho(W)$).
\end{itemize}

\paragraph{Taux de fuite}
Le troisième paramètre primordial des ESN, le taux de fuite peut être compris comme la vitesse de la dynamique de mise à jour du réservoir, mais aussi comme l'interval de temps entre deux consecutives \textit{timestep} de la réalisation discrète.\newline
D'après \cite{Lukosevicius12}:
\begin{itemize}
\item Le taux de fuite $\alpha$ doit correspondre à la vitesse de la dynamique de $u(n)$ et/ou $y^{target}(n)$.
\end{itemize}


\bibliographystyle{apalike}
\bibliography{reservoir}

\end{document}